import json
import re
import string

import numpy as np
import pandas as pd

from keras.preprocessing.text import Tokenizer, one_hot
from keras.preprocessing.sequence import pad_sequences
import argparse
from numpy import random, vstack, save, zeros
from gensim.models import Word2Vec
# import logging
import pickle

'''
Data Handler for TCGA Dataset

Paramerter:
--data_dir : Data directory with pathology text files
--label_dir : Label Split File Directory with files generated by trainTestSplitMetaData.py
--mapper_dir : Label to Integer Mapper Directory with both tasks : site and histology

Word2Vec Parameters:
--word2vec_min_count : Word2Vec Minimum Count

Methods:
get_split_docs()
prep_splits_data(args, split_labels, site_info)
clearup(document)
prep_data_CNN(documents)
word2Vec(docs,word_index)

'''



def parse_arguments():
    parser = argparse.ArgumentParser(description='args for the deep classifiers',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--data_dir', '-dr', default="data/features_full", type=str,
                        help='Provide the path for PathReports')
    parser.add_argument('--label_dir','-dr',default='data/split',type=str, help='Provide Label Splits as created earlier')
    parser.add_argument('--mapper_dir','-dr',default='data/mapper',type=str,help='Label to Integer Mapper Location')
    args = parser.parse_args()
    print(args)

    return args


args = parse_arguments()
# Word2Vec Parameters
word2vec_min_count = 5

# Splits the documents based on Train, Val and Test Label Split in  trainTestSplitMetaData.py
def get_split_docs(args):
    train_df = pd.read_csv(args.label_dir+'/train_labels.csv', delimiter=',')
    val_df = pd.read_csv(args.label_dir+'/val_labels.csv',delimiter=',')
    test_df = pd.read_csv(args.label_dir+'/test_labels.csv', delimiter=',')
    test_df.index = test_df.filename.values
    val_df.index = val_df.filename.values
    train_df.index = train_df.filename.values
    tr_site_info, te_site_info, tv_site_info = [], [], []
    tr_histology_info, te_histology_info, tv_histology_info = [], [], []

    # Site Mapper Contains String Label with Integer Label for Site and Histology
    class_site_mapper = json.load(open(args.mapper_dir+'/site_class_mapper.json', 'r'))
    tr_site_info = []
    for tsite in train_df.site.values:
        # print(tsite)
        tr_site_info.append(class_site_mapper[str(tsite).strip()])
    tv_site_info = []
    for vsite in val_df.site.values:
        tv_site_info.append(class_site_mapper[str(vsite).strip()])
    te_site_info = []
    for tesite in test_df.site.values:
        te_site_info.append(class_site_mapper[str(tesite).strip()])

    class_histology_mapper = json.load(open(args.mapper_dir+'/histology_class_mapper.json', 'r'))
    tr_histology_info = []
    for tsite in train_df.histology.values:
        # print(tsite)
        tr_histology_info.append(class_histology_mapper[str(tsite).strip()])
    tv_histology_info = []
    for vsite in val_df.histology.values:
        tv_histology_info.append(class_histology_mapper[str(vsite).strip()])
    te_histology_info = []
    for tesite in test_df.histology.values:
        te_histology_info.append(class_histology_mapper[str(tesite).strip()])

    tr_docs = prep_splits_data(args, train_df, tr_site_info)
    te_docs = prep_splits_data(args, test_df, te_site_info)
    tv_docs = prep_splits_data(args, val_df, tv_site_info)

    tr_info = list(zip(tr_site_info,tr_histology_info))
    tv_info = list(zip(tv_site_info,tv_histology_info))
    te_info = list(zip(te_site_info,te_histology_info))

    return tr_docs, te_docs,tv_docs, tr_info, te_info,tv_info

# Reads each data file for given split  and calls the preprocessing

def prep_splits_data(args, split_labels, site_info):
    documents = []
    df = split_labels
    for i in range(df.shape[0]):
        if '//' in str(df.index[i]):
            filename = df.index[i].split('//')[1].strip() + '.txt.hstlgy'
        else:
            filename = df.index[i].strip()
        fname = args.data_dir + "/" + filename.split('.hstlgy')[0].strip()
        doc = open(fname, 'r', encoding="utf8").read().strip()
        doc = clearup(doc)
        documents.append(doc)
    return documents


# Preprocessing

def clearup(document):
    document = document.translate(string.punctuation)
    numbers = re.search('[0-9]+', document)
    document = re.sub('\(\d+.\d+\)|\d-\d|\d', '', document) \
        .replace('.', '').replace(',', '').replace(',', '').replace(':', '').replace('~', '') \
        .replace('!', '').replace('@', '').replace('#', '').replace('$', '').replace('/', '') \
        .replace('%', '').replace('(', '').replace(')', '').replace('?', '') \
        .replace('â€”', '').replace(';', '').replace('&quot', '').replace('&lt', '') \
        .replace('^', '').replace('"', '').replace('{', '').replace('}', '').replace('\\', '').replace('+', '') \
        .replace('&gt', '').replace('&apos', '').replace('*', '').strip().lower().split()
    # return re.sub('[l]+', ' ', str(document)).strip()
    return document


def size(alist):
    return len(alist)

# Preparing data with padding , one hot encoded labels and vocabulary for CNN Input

def prep_data_CNN(documents):
    """
    Prepare the padded docs and vocab_size for CNN training
    """
    t = Tokenizer()
    docs = list(filter(None, documents))
    print("Size of the documents in prep_data {}".format(len(documents)))
    t.fit_on_texts(docs)

    vocab_size = len(t.word_counts)
    print("Vocab size {}".format(vocab_size))
    encoded_docs = t.texts_to_sequences(docs)
    print("Size of the encoded documents {}".format(len(encoded_docs)))
    e_lens = []
    for i in range(len(encoded_docs)):
        e_lens.append(len(encoded_docs[i]))
    lens_edocs = list(map(size, encoded_docs))
    max_length = np.average(lens_edocs)
    sequence_length = 1500  # Can use this instead of the above average max_length value
    max_length = sequence_length
    padded_docs = pad_sequences(
        encoded_docs, maxlen=int(max_length), padding='post')
    print("Length of a padded row {}".format(padded_docs.shape))
    print("max_length {} and min_length {} and average {}".format(
        max_length, min(lens_edocs), np.average(lens_edocs)))
    return padded_docs, max_length, vocab_size, t.word_index

# Word2Vec Embedding for HiSAN 

def word2Vec(docs,word_index):

    # train word2vec
    sentences = docs
    model = Word2Vec(sentences, min_count=word2vec_min_count, size=300, workers=4, iter=10)

    # save all word embeddings to matrix
    vocab_size = len(model.wv.vocab)
    vocab = zeros((vocab_size + 1, 300))
    word2idx = {}
    for i,(key,val) in enumerate(model.wv.vocab.items()):
        if key in word_index:
            word2idx[key] = i+1
            vocab[i+1, :] = model[key]

    # add additional word embedding for unknown words
    unk = len(vocab)
    vocab = vstack((vocab, random.rand(1, 300) - 0.5))

    # normalize embeddings
    vocab -= vocab.mean()
    vocab /= (vocab.std() * 2.5)
    vocab[0, :] = 0
    max_len = 1500

    # convert words to indices
    text_idx = zeros((len(sentences), max_len))
    for i, sent in enumerate(sentences):
        idx = [word2idx[word] if word in word2idx else unk for word in sent][:max_len]
        l = len(idx)
        text_idx[i, :l] = idx

    # save data
    return text_idx,word2idx,vocab

if __name__ == '__main__':
    
    tr_docs, te_docs, tv_docs, y_train, y_test, y_val = get_split_docs(args)
    prep_docs = tr_docs + tv_docs + te_docs
    padded_docs, max_length, vocab_size, word_index = prep_data_CNN(prep_docs)
    padded_docs, word2idx, vocab = word2Vec(prep_docs,word_index)

    # word2idx - Word to Index Dictionary
    # vocab - Complete Vocabulary File
    with open('data/word2idx.pkl', 'wb') as f:
        pickle.dump(word2idx, f)
    save('data/vocab.npy', vocab)
    train_x = padded_docs[:len(tr_docs)]
    val_x = padded_docs[len(tr_docs):len(tr_docs) + len(tv_docs)]
    test_x = padded_docs[len(tr_docs) + len(tv_docs):]
    
    # CNN Prepped Train, Val and Test numpys
    np.save("data/npy/train_X.npy",train_x)
    np.save("data/npy/test_X.npy",test_x)
    np.save("data/npy/val_X.npy",val_x)
    np.save("data/npy/train_Y.npy",np.array(y_train))
    np.save("data/npy/test_Y.npy", np.array(y_test))
    np.save("data/npy/val_Y.npy", np.array(y_val))